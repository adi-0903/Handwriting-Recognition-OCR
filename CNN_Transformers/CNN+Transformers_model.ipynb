{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEgCUDFGgfef"
   },
   "source": [
    "# Baseline Model for Handwritten recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlK-Y_W-goNT"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FcT8pKLmgUHh"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, Reshape, Dense, Input, Cropping1D, RepeatVector, Flatten\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Dropout, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.random import uniform\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "miyZX375x_O3"
   },
   "outputs": [],
   "source": [
    "#loading configuration file\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxKX25_i3bKJ"
   },
   "source": [
    "## Loading Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-kDv2YXtyIWN",
    "outputId": "c7a87055-1f91-4ab9-97f4-a5cb88974c1c"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#The currect resized images are BW images edited in the preprocessing stage."
   ],
   "metadata": {
    "id": "OGg1Mky00yik"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "y2C754k23Yem",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a274f981-1b8a-4180-be7b-4d50372538a8"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(13353, 64, 512)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "folder_path = 'drive/MyDrive/resized_images'\n",
    "\n",
    "images = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "  image_path = os.path.join(folder_path, file_name)\n",
    "  image = np.array(Image.open(image_path))\n",
    "  images.append(image)\n",
    "\n",
    "images = np.array(images)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wfg2bR3UxBQ8",
    "outputId": "48a699a8-3191-4e29-91d8-4f14b3991f71"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(13353,)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "labels = pd.read_csv('labels.csv', header=None).iloc[:,1]\n",
    "labels.name = 'labels'\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample data\n",
    "Relevant for fast training. need to adjust the following stages of setting data for the model in order to run with sample instead full data."
   ],
   "metadata": {
    "id": "btzC2cqLw1Yx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# sample set of 500\n",
    "# !unzip 'test_bw.zip'"
   ],
   "metadata": {
    "id": "M_u6Ly91lLY8"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# sampled_labels = labels[:500]\n",
    "\n",
    "# folder_path = 'test_bw'\n",
    "\n",
    "# sampled_images = []\n",
    "# for file_name in os.listdir(folder_path):\n",
    "#   image_path = os.path.join(folder_path, file_name)\n",
    "#   image = np.array(Image.open(image_path))\n",
    "#   sampled_images.append(image)\n",
    "\n",
    "# sampled_images = np.array(sampled_images)\n",
    "# sampled_images.shape"
   ],
   "metadata": {
    "id": "nztHX6VnlaBT"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Splitting the data"
   ],
   "metadata": {
    "id": "2-bOevOFwxiF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "WLlGvUGcx_Ox"
   },
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "wLwSw5WHiCxm"
   },
   "outputs": [],
   "source": [
    "def label_preprocessing(labels_sentence, vocabulary):\n",
    "    '''\n",
    "    Takes in a single label as a string (sentence matching the content of the image) and\n",
    "    preprocesses it so that the label can interpreted by the model\n",
    "    Param: label_sentence: the label a string\n",
    "    Returns: The preprocessed label as a sequence of indexes\n",
    "    '''\n",
    "    max_sequence_length = max(len(sentence) for sentence in labels_sentence )\n",
    "\n",
    "    labels_indexes = [[vocabulary[char] for char in sentence] for sentence in labels_sentence]\n",
    "    preprocesed_label = pad_sequences(labels_indexes, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    return preprocesed_label, max_sequence_length\n",
    "\n",
    "\n",
    "def get_vocabulary(train_labels):\n",
    "    \"\"\"\n",
    "    creates a vocabulary for the translation of the prediction\n",
    "    \"\"\"\n",
    "    vocabulary = sorted(set(''.join(train_labels)))\n",
    "    vocabulary_dict = {char: index for index, char in enumerate(vocabulary)}\n",
    "    return vocabulary_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preparing the data to be numpy array of character tokens. Calculaing max_sequence_length."
   ],
   "metadata": {
    "id": "0tzyLmPow6Wz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "whiDXvxpmT59"
   },
   "outputs": [],
   "source": [
    "vocabulary = get_vocabulary(labels)\n",
    "train_labels_preprocessed, max_sequence_length = label_preprocessing(train_labels, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ammgrmlgx_O1"
   },
   "source": [
    "## Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "iMRZhc7tx_O2"
   },
   "outputs": [],
   "source": [
    "def wer(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    returns calculation of Word Error Rate for a prediction.\n",
    "    \"\"\"\n",
    "    wer_value = wer(y_true, y_pred)\n",
    "    return wer_value\n",
    "\n",
    "def cer(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    returns calculation of Character Error Rate for a prediction.\n",
    "    \"\"\"\n",
    "    matcher = difflib.SequenceMatcher(None, y_true, y_pred)\n",
    "    cer_value = 1 - matcher.ratio()\n",
    "    return cer_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uB04KDyh-ZJ"
   },
   "source": [
    "## Baseline Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "uvpKJ_0yx_O3"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "input_shape = (images.shape[1],\n",
    "               images.shape[2],\n",
    "               1)\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-4, clipvalue=1)\n",
    "loss = SparseCategoricalCrossentropy()\n",
    "num_classes = len(vocabulary) + 1 # additional 1 for CTC blank value"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CTC loss function (or alternative CTC layer)"
   ],
   "metadata": {
    "id": "SS9_gfffwY4J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a CTC layer implementation, that outputs max_len_sentence chracters. currently it's not in use, and instead we use CTCloss as a loss function directly in the compilation stage."
   ],
   "metadata": {
    "id": "Jquy3CLUzxxW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# class CTCLayer(layers.Layer):\n",
    "#     def __init__(self, name=None):\n",
    "#         super().__init__(name=name)\n",
    "#         self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "#         input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "#         label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "\n",
    "#         input_length *= tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "#         label_length *= tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "\n",
    "#         loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "#         self.add_loss(loss)\n",
    "\n",
    "#         return y_pred"
   ],
   "metadata": {
    "id": "BFIWyvriuO1g"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CTCloss(tf.keras.losses.Loss):\n",
    "    \"\"\" CTCLoss objec for training the model\"\"\"\n",
    "    def __init__(self, name: str = \"CTCloss\") -> None:\n",
    "        super(CTCloss, self).__init__()\n",
    "        self.name = name\n",
    "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
    "\n",
    "    def __call__(self, y_true: tf.Tensor, y_pred: tf.Tensor, sample_weight=None) -> tf.Tensor:\n",
    "        \"\"\" Compute the training batch CTC loss value\"\"\"\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "id": "KljDeQp21sAI"
   },
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN+Transformers Model implementation"
   ],
   "metadata": {
    "id": "Xh6CAL50wRMs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model is based on paper called 'Optical Character Recognition with Transformers and CTC'. The model contains currently 5 Conv2D layers, followed by 3 encoder BERT like Transformer layers."
   ],
   "metadata": {
    "id": "Db7BXMNM0I7r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define input layer\n",
    "inputs = Input(shape=input_shape, name='Input')\n",
    "\n",
    "# Define CNN layers, the filters, num of layers, and strides might change.\n",
    "x = Conv2D(64, (3, 3), strides=(2,2), activation='relu', name='conv1')(inputs)\n",
    "x = Conv2D(128, (3, 3), strides=(2,2), activation='relu', name='conv2')(x)\n",
    "x = Conv2D(256, (3, 3), strides=(2,1), activation='relu', name='conv3')(x)\n",
    "x = Conv2D(512, (3, 3), strides=(4,1), activation='relu', name='conv4')(x)\n",
    "x = Conv2D(512, (2, 2), strides=(1,1), activation='relu', name='conv5')(x)\n",
    "\n",
    "# Reshape output of Convolutional layers to BERT like shape.\n",
    "x = Reshape((-1, 512), name='Reshape')(x)\n",
    "\n",
    "for _ in range(config[\"num_transformer_layers\"]):\n",
    "    # Add Multi-Head Attention layer\n",
    "    x = LayerNormalization(epsilon=1e-6, name=f'LayerNomalization{_*2+1}')(x)\n",
    "    x = MultiHeadAttention(num_heads=config[\"num_heads\"], key_dim=8, name=f'MultiHeadAttention{_+1}')(x, x)\n",
    "    x = Dropout(config[\"dropout_rate\"], name=f'Dropout{_*2+1}')(x)\n",
    "\n",
    "    # Add Feed Forward Neural Network layer\n",
    "    ffn_output = Dense(512, activation='relu', name=f'Dense{_+1}')(x)\n",
    "    x = LayerNormalization(epsilon=1e-6, name=f'LayerNomalization{_*2+2}')(x + ffn_output)\n",
    "    x = Dropout(config[\"dropout_rate\"], name=f'Dropout{_*2+2}')(x)\n",
    "\n",
    "# Output Dense layer\n",
    "x = Dense(units=num_classes, activation='softmax', name='Dense4')(x)\n",
    "\n",
    "#Cropping Data to change output to max_length_sentence length\n",
    "output = Cropping1D(cropping=(0, 29))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output, name=\"ocr_model_CNN_Transformers_CTC\")\n",
    "\n",
    "# # Structure for alternative CTC. In that case the model input includes (Inputs, labels) .\n",
    "\n",
    "# labels = Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "# output = CTCLayer(name=\"ctc_loss\")(labels, output)\n",
    "# model = tf.keras.Model(inputs=[inputs, labels], outputs=output, name=\"ocr_model_CNN_Transformers_CTC\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=CTCloss())"
   ],
   "metadata": {
    "id": "gOK6at4Ozjon"
   },
   "execution_count": 74,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sL8OYdPLeBs"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Fit the data and observe the kernels and biases of the layers."
   ],
   "metadata": {
    "id": "bjzBNqqc0gVX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbYKYKg9LWUj"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1, write_grads=True)\n",
    "\n",
    "model.fit(x=(train_images, train_labels_preprocessed),\n",
    "          validation_split = config[\"VAL_SPLIT\"],\n",
    "          epochs=40,\n",
    "          batch_size=32,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs\n"
   ],
   "metadata": {
    "id": "_7ZxsC32gTT4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3f90GCRx_O7"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3qfIsYOx_O7"
   },
   "outputs": [],
   "source": [
    "preds = model.predict((test_images, test_labels))\n",
    "index_to_char = {v: k for k, v in vocabulary.items()}\n",
    "cer_sum = 0\n",
    "wer_sum = 0\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    indices = np.argmax(pred, axis=-1)\n",
    "\n",
    "    print(len(indices))\n",
    "\n",
    "    characters = ''.join([index_to_char[idx] for idx in indices])\n",
    "    cer_sum += character_error_rate(test_labels[i], characters)\n",
    "    wer_sum += word_error_rate(test_labels[i], characters)\n",
    "\n",
    "print('CER mean: ', cer_sum / len(test_labels))\n",
    "print('WER mean: ', wer_sum / len(test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
